{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhkdsns20/AI-Basic/blob/main/C2_W3_Lab_1_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__vdUT_B1O-I"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C2/W3/ungraded_lab/C2_W3_Lab_1_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT0to3TL2q7H"
      },
      "source": [
        "# Ungraded Lab: Transfer Learning\n",
        "\n",
        "In this lab, you will see how you can use a pre-trained model to achieve good results even with a small training dataset. This is called _transfer learning_ and you do this by leveraging the trained layers of an existing model and adding your own layers to fit your application. For example, you can:\n",
        "\n",
        "1. just get the convolution layers of one model\n",
        "2. attach some dense layers onto it\n",
        "3. train just the dense network\n",
        "4. evaluate the results\n",
        "\n",
        "Doing this will allow you to save time building your application because you will essentially skip weeks of training time of very deep networks. You will just use the features it has learned and tweak it for your dataset. Let's see how these are done in the next sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvrr8pLRzJMV"
      },
      "source": [
        "**IMPORTANT NOTE:** This notebook is designed to run as a Colab. Running the notebook on your local machine might result in some of the code blocks throwing errors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ìš”ì•½: ì „ì´ í•™ìŠµ ì‹¤ìŠµ (Ungraded Lab: Transfer Learning)\n",
        "ì´ ì‹¤ìŠµì—ì„œëŠ” ì „ì´ í•™ìŠµ(Transfer Learning) ì„ í†µí•´ ì†ŒëŸ‰ì˜ ë°ì´í„°ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. ì „ì´ í•™ìŠµì€ ê¸°ì¡´ì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(ì˜ˆ: InceptionV3) ì„ í™œìš©í•˜ì—¬ í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "0ai8QndHRCSn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-12slkPL6_JH"
      },
      "source": [
        "## Setup the pretrained model\n",
        "\n",
        "You will need to prepare pretrained model and configure the layers that you need. For this exercise, you will use the convolution layers of the [InceptionV3](https://arxiv.org/abs/1512.00567) architecture as your base model. To do that, you need to:\n",
        "\n",
        "1. Set the input shape to fit your application. In this case. set it to `150x150x3` as you've been doing in the last few labs.\n",
        "\n",
        "2. Pick and freeze the convolution layers to take advantage of the features it has learned already.\n",
        "\n",
        "3. Add dense layers which you will train.\n",
        "\n",
        "Let's see how to do these in the next cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VqhFEK2Y-PK"
      },
      "source": [
        "First, in preparing the input to the model, you want to fetch the pretrained weights of the `InceptionV3` model and remove the fully connected layer at the end because you will be replacing it later. You will also specify the input shape that your model will accept. Lastly, you want to freeze the weights of these layers because they have been trained already."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìš”ì•½: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì„¤ì • (InceptionV3 ê¸°ë°˜ ì „ì´ í•™ìŠµ ì¤€ë¹„)\n",
        "ì´ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ì „ í•™ìŠµëœ InceptionV3 ëª¨ë¸ì˜ í•©ì„±ê³± ì¸µë§Œ ê°€ì ¸ì™€ì„œ ìƒˆë¡œìš´ ì‘ì—…ì— ë§ê²Œ ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "ìˆ˜í–‰í•  ì‘ì—…\n",
        "1. ì…ë ¥ í¬ê¸° ì„¤ì •\n",
        "\n",
        "  (150, 150, 3) : RGB ì´ë¯¸ì§€ë¡œ ê³ ì • (ì´ì „ ì‹¤ìŠµê³¼ ë™ì¼)\n",
        "\n",
        "2. ì‚¬ì „ í•™ìŠµëœ InceptionV3 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "\n",
        "  include_top=False â†’ ë§ˆì§€ë§‰ Fully Connected(FC) ì¸µ ì œê±°\n",
        "\n",
        "  weights='imagenet' â†’ ImageNetìœ¼ë¡œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©\n",
        "\n",
        "3. í•©ì„±ê³± ì¸µ ê³ ì • (Freeze)\n",
        "\n",
        "  ê¸°ì¡´ì— í•™ìŠµëœ í•©ì„±ê³± ê³„ì¸µë“¤ì€ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ì•Šë„ë¡ ê³ ì •\n",
        "\n",
        "4. ìƒˆë¡œìš´ Dense ì¸µ ì¶”ê°€\n",
        "\n",
        "  ì´í›„ ë‹¨ê³„ì—ì„œ í•™ìŠµí•  ìƒˆë¡œìš´ ì¶œë ¥ì¸µì„ ì¶”ê°€"
      ],
      "metadata": {
        "id": "sAD3d8rSRF1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xJZ5glPPCRz"
      },
      "outputs": [],
      "source": [
        "# Download the pre-trained weights. No top means it excludes the fully connected layer it uses for classification.\n",
        "# ì‚¬ì „ í•™ìŠµëœ InceptionV3 ëª¨ë¸ì˜ í•©ì„±ê³± ì¸µë§Œ ê°€ì ¸ì™€ì„œ ìƒˆë¡œìš´ ì‘ì—…ì— ë§ê²Œ ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "jXANhRwdND73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsiBCpQ1VvPp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "# InceptionV3 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (FC ì¸µ ì œì™¸, ê°€ì¤‘ì¹˜ëŠ” ë‚˜ì¤‘ì— ë¡œë“œ)\n",
        "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
        "                                 include_top=False,\n",
        "                                 weights=None)\n",
        "\n",
        "# ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "# ëª¨ë“  ë ˆì´ì–´ì˜ í•™ìŠµì„ ë¹„í™œì„±í™” (ê³ ì •)\n",
        "for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y2rEnqFaa9k"
      },
      "source": [
        "You can see the summary of the model below. You can see that it is a very deep network. You can then select up to which point of the network you want to use. As Laurence showed in the exercise, you will use up to `mixed7` as your base model and add to that. This is because the original last layer might be too specialized in what it has learned so it might not translate well into your application. `mixed7` on the other hand will be more generalized and you can start with that for your application. After the exercise, feel free to modify and use other layers to see what the results you get.\n",
        "\n",
        "ì‚¬ì „ í•™ìŠµëœ InceptionV3 ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ìš”ì•½(summary) í•´ë³´ê³ , ê·¸ ì¤‘ì—ì„œ ì–´ë””ê¹Œì§€ ì‚¬ìš©í• ì§€ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” mixed7 ë ˆì´ì–´ê¹Œì§€ë¥¼ ê¸°ë°˜ ëª¨ë¸ì˜ ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì£¼ìš” í¬ì¸íŠ¸\n",
        "- InceptionV3ëŠ” ë§¤ìš° ê¹Šì€(deep) ì‹ ê²½ë§ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ\n",
        "\n",
        "- ë§ˆì§€ë§‰ ì¶œë ¥ì¸µì€ ImageNet ë°ì´í„°ì— íŠ¹í™”ëœ íŠ¹ì„±ì„ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì—, ë‹¤ë¥¸ ë¬¸ì œì— ê³¼ì í•©(overfitting) ë  ìˆ˜ ìˆìŒ\n",
        "\n",
        "- mixed7 ë ˆì´ì–´ëŠ” ë¹„êµì  ì¼ë°˜ì ì¸ íŠ¹ì§•(generalized features) ì„ ì¶”ì¶œí•˜ëŠ” ìœ„ì¹˜ì´ê¸° ë•Œë¬¸ì—, ì „ì´ í•™ìŠµì˜ ì¶œë°œì ìœ¼ë¡œ ì ì ˆí•¨\n",
        "\n",
        "ì°¸ê³  íŒ\n",
        "- mixed7 ì´ì „ê¹Œì§€ì˜ ì¸µì€ ì´ë¯¸ì§€ì˜ ì¼ë°˜ì ì¸ íŒ¨í„´(ëª¨ì„œë¦¬, ì§ˆê° ë“±)ì„ í•™ìŠµ\n",
        "\n",
        "- ê·¸ ì´í›„ì˜ ì¸µì€ ImageNetì˜ 1000ê°œ í´ë˜ìŠ¤ì— íŠ¹í™”ëœ ê³ ì°¨ì› íŒ¨í„´ì„ í•™ìŠµí•¨\n",
        "\n",
        "- ë”°ë¼ì„œ ì¤‘ê°„ ì§€ì ì¸ mixed7ê¹Œì§€ë§Œ ì‚¬ìš©í•˜ê³ , ê·¸ ì´í›„ëŠ” ìƒˆë¡œìš´ ë¬¸ì œì— ë§ëŠ” ì¸µìœ¼ë¡œ ì§ì ‘ êµ¬ì„±í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeGP0Ust5kCR"
      },
      "outputs": [],
      "source": [
        "pre_trained_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDmGO9tg5iPc"
      },
      "outputs": [],
      "source": [
        "# InceptionV3 ëª¨ë¸ì—ì„œ 'mixed7' ë ˆì´ì–´ë¥¼ ì„ íƒ\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "\n",
        "# ì„ íƒí•œ ë ˆì´ì–´ì˜ ì¶œë ¥ í…ì„œ í˜•íƒœ í™•ì¸\n",
        "print('last layer output shape: ', last_layer.output.shape)\n",
        "\n",
        "# í•´ë‹¹ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ëª¨ë¸ êµ¬ì„± ì˜ˆì •\n",
        "last_output = last_layer.output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXT9SDMK7Ioa"
      },
      "source": [
        "## Add dense layers for your classifier\n",
        "\n",
        "Next, you will add dense layers to your model. These will be the layers that you will train and is tasked with recognizing cats and dogs. You will add a [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layer as well to regularize the output and avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMXb913pbvFg"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (1, activation='sigmoid')(x)\n",
        "\n",
        "# Append the dense network to the base model\n",
        "model = Model(pre_trained_model.input, x)\n",
        "\n",
        "# Print the model summary. See your dense network connected at the end.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAwTTkWr56uC"
      },
      "outputs": [],
      "source": [
        "# Set the training parameters\n",
        "model.compile(optimizer = RMSprop(learning_rate=0.0001),\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYLGw_RO7Z_X"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "Now you will prepare the dataset. This is basically the same code as the one you used in the data augmentation lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4s8HckqGlnb"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/cats_and_dogs_filtered.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 1. Flatten ì¸µ: 7x7x768 â†’ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜\n",
        "x = layers.Flatten()(last_output)\n",
        "\n",
        "# 2. Dense ì¸µ: ì€ë‹‰ì¸µ (ReLU í™œì„±í™”)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "\n",
        "# 3. ì¶œë ¥ì¸µ: sigmoid í™œì„±í™” â†’ ì´ì§„ ë¶„ë¥˜ (1ê°œì˜ ì¶œë ¥ê°’)\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# 4. ëª¨ë¸ êµ¬ì„±: ì…ë ¥ì€ pre_trained_modelì˜ ì…ë ¥, ì¶œë ¥ì€ ìƒˆë¡œ ë§Œë“  x\n",
        "model = Model(pre_trained_model.input, x)\n",
        "\n",
        "# 5. ëª¨ë¸ ì»´íŒŒì¼\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])  # ì •í™•ë„ ì¶”ì \n"
      ],
      "metadata": {
        "id": "nK1T0MX5PnpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOV8jON3c3Jv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# ğŸ“¦ 1. ë°ì´í„° ì••ì¶• íŒŒì¼(zip) ì—´ê¸° ë° í•´ì œ\n",
        "zip_ref = zipfile.ZipFile(\"./cats_and_dogs_filtered.zip\", 'r')  # zip íŒŒì¼ ì—´ê¸°\n",
        "zip_ref.extractall(\"tmp/\")  # 'tmp/' í´ë”ì— ì••ì¶• í•´ì œ\n",
        "zip_ref.close()  # íŒŒì¼ ë‹«ê¸°\n",
        "\n",
        "# ğŸ“ 2. ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
        "base_dir = 'tmp/cats_and_dogs_filtered'  # ê¸°ë³¸ ë°ì´í„° í´ë” ê²½ë¡œ\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')  # í›ˆë ¨ ë°ì´í„° ê²½ë¡œ\n",
        "validation_dir = os.path.join(base_dir, 'validation')  # ê²€ì¦ ë°ì´í„° ê²½ë¡œ\n",
        "\n",
        "# ğŸ± ê³ ì–‘ì´/ê°•ì•„ì§€ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì • (í›ˆë ¨ìš©)\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')  # í›ˆë ¨ìš© ê³ ì–‘ì´ ì´ë¯¸ì§€ í´ë”\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')  # í›ˆë ¨ìš© ê°•ì•„ì§€ ì´ë¯¸ì§€ í´ë”\n",
        "\n",
        "# ğŸ± ê³ ì–‘ì´/ê°•ì•„ì§€ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì • (ê²€ì¦ìš©)\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')  # ê²€ì¦ìš© ê³ ì–‘ì´ ì´ë¯¸ì§€ í´ë”\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # ê²€ì¦ìš© ê°•ì•„ì§€ ì´ë¯¸ì§€ í´ë”\n",
        "\n",
        "# ğŸ” 3. í›ˆë ¨ ë°ì´í„°ìš© ImageDataGenerator ì •ì˜ (ë°ì´í„° ì¦ê°• í¬í•¨)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255.,              # í”½ì…€ê°’ì„ 0~1 ì‚¬ì´ë¡œ ì •ê·œí™”\n",
        "    rotation_range=40,           # ë¬´ì‘ìœ„ íšŒì „\n",
        "    width_shift_range=0.2,       # ìˆ˜í‰ ì´ë™\n",
        "    height_shift_range=0.2,      # ìˆ˜ì§ ì´ë™\n",
        "    shear_range=0.2,             # ê¸°ìš¸ì´ê¸° ë³€í™˜\n",
        "    zoom_range=0.2,              # í™•ëŒ€/ì¶•ì†Œ\n",
        "    horizontal_flip=True         # ìˆ˜í‰ ë’¤ì§‘ê¸°\n",
        ")\n",
        "\n",
        "# â— ê²€ì¦ ë°ì´í„°ëŠ” ì¦ê°• ì—†ì´ ì •ê·œí™”ë§Œ ìˆ˜í–‰\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
        "\n",
        "# ğŸš€ 4. í›ˆë ¨ìš© ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,                   # í›ˆë ¨ ë°ì´í„° í´ë” ê²½ë¡œ\n",
        "    batch_size=20,              # ë°°ì¹˜ í¬ê¸°\n",
        "    class_mode='binary',        # ì´ì§„ ë¶„ë¥˜ì´ë¯€ë¡œ binary ëª¨ë“œ\n",
        "    target_size=(150, 150)      # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n",
        ")\n",
        "\n",
        "# ğŸš€ 5. ê²€ì¦ìš© ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,             # ê²€ì¦ ë°ì´í„° í´ë” ê²½ë¡œ\n",
        "    batch_size=20,              # ë°°ì¹˜ í¬ê¸°\n",
        "    class_mode='binary',        # ì´ì§„ ë¶„ë¥˜\n",
        "    target_size=(150, 150)      # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m3S6AZb7h-B"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "With that, you can now train the model. You will do 20 epochs and plot the results afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Blhq2MAUeyGA"
      },
      "outputs": [],
      "source": [
        "# ğŸ§ª ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
        "history = model.fit(\n",
        "    train_generator,             # í›ˆë ¨ìš© ì´ë¯¸ì§€ ì œë„ˆë ˆì´í„°\n",
        "    validation_data=validation_generator,  # ê²€ì¦ìš© ì´ë¯¸ì§€ ì œë„ˆë ˆì´í„°\n",
        "    steps_per_epoch=100,        # í•œ ì—í­ë‹¹ í›ˆë ¨ ë°°ì¹˜ ìˆ˜ (ì˜ˆ: ì´ 2000ì¥ ì´ë¯¸ì§€ / ë°°ì¹˜ 20 = 100 ìŠ¤í…)\n",
        "    epochs=20,                   # ì´ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜\n",
        "    validation_steps=50,        # í•œ ì—í­ë‹¹ ê²€ì¦ ë°°ì¹˜ ìˆ˜ (ì˜ˆ: ì´ 1000ì¥ ì´ë¯¸ì§€ / ë°°ì¹˜ 20 = 50 ìŠ¤í…)\n",
        "    verbose=2                    # í•™ìŠµ ì¤‘ ì¶œë ¥ ëª¨ë“œ (2ëŠ” ì—í­ ë‹¨ìœ„ë¡œ ê°„ê²°í•˜ê²Œ ì¶œë ¥)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwcB2bPj7lIx"
      },
      "source": [
        "## Evaluate the results\n",
        "\n",
        "You will use the same code to plot the results. As you can see, the validation accuracy is also trending upwards as your training accuracy improves. This is a good sign that your model is no longer overfitting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Fp6Se9rKuL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# í•™ìŠµ ì´ë ¥(history)ì—ì„œ ì •í™•ë„ì™€ ì†ì‹¤ ê°’ ì¶”ì¶œ\n",
        "acc = history.history['acc']           # í›ˆë ¨ ì •í™•ë„\n",
        "val_acc = history.history['val_acc']   # ê²€ì¦ ì •í™•ë„\n",
        "loss = history.history['loss']         # í›ˆë ¨ ì†ì‹¤\n",
        "val_loss = history.history['val_loss'] # ê²€ì¦ ì†ì‹¤\n",
        "\n",
        "# ì—í­ ìˆ˜ì— ëŒ€í•œ ì¸ë±ìŠ¤ ìƒì„± (ì˜ˆ: range(20) â†’ 0~19)\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# ğŸ¯ ì •í™•ë„ ì‹œê°í™”\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')        # í›ˆë ¨ ì •í™•ë„ (ë¹¨ê°„ìƒ‰)\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')  # ê²€ì¦ ì •í™•ë„ (íŒŒë€ìƒ‰)\n",
        "plt.title('Training and Validation Accuracy')  # ê·¸ë˜í”„ ì œëª©\n",
        "plt.legend(loc=0)  # ë²”ë¡€ ìœ„ì¹˜ ìë™ ì„¤ì •\n",
        "plt.figure()  # ìƒˆ ê·¸ë˜í”„ ìƒì„±\n",
        "\n",
        "# ğŸ¯ ì†ì‹¤ ì‹œê°í™”\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')        # í›ˆë ¨ ì†ì‹¤\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')  # ê²€ì¦ ì†ì‹¤\n",
        "plt.title('Training and Validation Loss')  # ê·¸ë˜í”„ ì œëª©\n",
        "plt.legend(loc=0)  # ë²”ë¡€ ìœ„ì¹˜ ìë™ ì„¤ì •\n",
        "\n",
        "# ğŸ“ˆ ê·¸ë˜í”„ ì¶œë ¥\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yi3QvoWEa-iK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "UXT9SDMK7Ioa"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}